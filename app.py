import json
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import streamlit as st

# OpenAI SDK v1+
try:
    from openai import OpenAI
    from openai import RateLimitError, APIError, APITimeoutError
except Exception:
    OpenAI = None
    RateLimitError = Exception
    APIError = Exception
    APITimeoutError = Exception


# ----------------------------
# Utils
# ----------------------------
def load_json(path: str) -> Dict[str, Any]:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {path}")
    return json.loads(p.read_text(encoding="utf-8"))


def normalize_text(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"\s+", " ", s)
    return s


def tokenize_ru(s: str) -> List[str]:
    s = normalize_text(s)
    # –ø—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–µ–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ (MVP)
    return re.findall(r"[–∞-—èa-z0-9]+", s, flags=re.IGNORECASE)


@dataclass
class Chunk:
    source: str
    idx: int
    text: str


def chunk_text(text: str, max_chars: int = 1800, overlap: int = 120) -> List[str]:
    text = text.strip()
    if not text:
        return []
    chunks = []
    i = 0
    n = len(text)
    while i < n:
        end = min(n, i + max_chars)
        chunk = text[i:end]
        chunks.append(chunk)
        if end == n:
            break
        i = max(0, end - overlap)
    return chunks


@st.cache_data(show_spinner=False)
def load_knowledge_chunks(knowledge_dir: str, files: List[str], max_chars: int, overlap: int) -> List[Chunk]:
    chunks: List[Chunk] = []
    kdir = Path(knowledge_dir)
    for fname in files:
        fp = kdir / fname
        if not fp.exists():
            # –ù–µ –ø–∞–¥–∞–µ–º: –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ debug –ø–æ–∑–∂–µ
            continue
        raw = fp.read_text(encoding="utf-8", errors="ignore")
        parts = chunk_text(raw, max_chars=max_chars, overlap=overlap)
        for idx, part in enumerate(parts):
            chunks.append(Chunk(source=fname, idx=idx, text=part))
    return chunks


def retrieve_chunks(chunks: List[Chunk], query: str, top_k: int, max_total_chars: int) -> Tuple[List[Chunk], List[Tuple[Chunk, float]]]:
    """
    –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫: —Å–∫–æ—Ä = –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –∏ —á–∞–Ω–∫–∞.
    –≠—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —Ç—è–Ω—É—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –Ω–µ –∂–µ—á—å —Ç–æ–∫–µ–Ω—ã.
    """
    q_tokens = set(tokenize_ru(query))
    scored: List[Tuple[Chunk, float]] = []
    for ch in chunks:
        c_tokens = set(tokenize_ru(ch.text))
        inter = q_tokens.intersection(c_tokens)
        score = float(len(inter))
        if score > 0:
            scored.append((ch, score))
    scored.sort(key=lambda x: x[1], reverse=True)

    picked: List[Chunk] = []
    total = 0
    for ch, _s in scored[: max(top_k * 5, top_k)]:  # —á—É—Ç—å —à–∏—Ä–µ, –ø–æ—Ç–æ–º —Ä–µ–∂–µ–º –ø–æ –ª–∏–º–∏—Ç—É —Å–∏–º–≤–æ–ª–æ–≤
        if total >= max_total_chars:
            break
        t = ch.text
        if total + len(t) > max_total_chars:
            t = t[: max(0, max_total_chars - total)]
            picked.append(Chunk(source=ch.source, idx=ch.idx, text=t))
            total = max_total_chars
            break
        picked.append(ch)
        total += len(t)

        if len(picked) >= top_k and total >= (max_total_chars * 0.7):
            break

    return picked, scored[:20]


def get_openai_client() -> Any:
    api_key = None
    # Streamlit Cloud: st.secrets
    if hasattr(st, "secrets"):
        api_key = st.secrets.get("OPENAI_API_KEY") or st.secrets.get("api_key")
    # env fallback (—Ä–µ–¥–∫–æ –Ω—É–∂–Ω–æ)
    if not api_key:
        api_key = st.session_state.get("_OPENAI_API_KEY")

    if not api_key:
        st.error("–ù–µ—Ç –∫–ª—é—á–∞ OpenAI. –î–æ–±–∞–≤—å OPENAI_API_KEY –≤ Streamlit Secrets.")
        st.stop()

    if OpenAI is None:
        st.error("–ü–∞–∫–µ—Ç openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è. –ü—Ä–æ–≤–µ—Ä—å requirements.txt (openai>=1.0.0).")
        st.stop()

    return OpenAI(api_key=api_key)


def safe_json_loads(s: str) -> Optional[Dict[str, Any]]:
    try:
        return json.loads(s)
    except Exception:
        # –∏–Ω–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç JSON –≤ —Ç–µ–∫—Å—Ç
        m = re.search(r"\{.*\}", s, flags=re.DOTALL)
        if not m:
            return None
        try:
            return json.loads(m.group(0))
        except Exception:
            return None


# ----------------------------
# LLM protocol (strict JSON)
# ----------------------------
SYSTEM_PROMPT = """
–¢—ã ‚Äî AI-–∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä, –ø—Ä–æ–≤–æ–¥–∏—à—å "–∂–∏–≤–æ–π —Ä–∞–∑–±–æ—Ä" –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–≤ (NEO).
–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –∑–∞–¥–∞–≤–∞—Ç—å –û–î–ò–ù —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –∑–∞ —Ä–∞–∑, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –∏ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–í–∞–∂–Ω–æ:
- –ù–ï –ø–æ–≤—Ç–æ—Ä—è–π –≤–æ–ø—Ä–æ—Å—ã (—Å–º–æ—Ç—Ä–∏ –∏—Å—Ç–æ—Ä–∏—é).
- –î–µ–ª–∞–π –≤–æ–ø—Ä–æ—Å—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏, –ø–æ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏, –∫–∞–∫ –º–∞—Å—Ç–µ—Ä –≤ —Ä–µ–∞–ª—å–Ω–æ–º —Ä–∞–∑–±–æ—Ä–µ.
- –ï—Å–ª–∏ –º–æ–∂–Ω–æ ‚Äî –¥–∞–≤–∞–π –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤ (radio/checkbox), —á—Ç–æ–±—ã –∫–ª–∏–µ–Ω—Ç—É –Ω–µ –ø—Ä–∏—à–ª–æ—Å—å –ø–∏—Å–∞—Ç—å –º–Ω–æ–≥–æ.
- –ï—Å–ª–∏ –Ω—É–∂–µ–Ω —Ç–µ–∫—Å—Ç ‚Äî –ø–æ–ø—Ä–æ—Å–∏ –∫–æ—Ä–æ—Ç–∫–æ + 1 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä.
- –£–≤–∞–∂–∞–π –∑–∞–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞ –∏ —É–¥–µ—Ä–∂–∏–≤–∞–π –Ω–∏—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.
- –ò–Ω–æ–≥–¥–∞ –¥–µ–ª–∞–π —É—Ç–æ—á–Ω–µ–Ω–∏—è –ø–æ –¥–µ—Ç—Å—Ç–≤—É/–ø–æ–≤–µ–¥–µ–Ω–∏—é/—ç–Ω–µ—Ä–≥–∏–∏/–∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º, –Ω–æ –±–µ–∑ "–∞–Ω–∫–µ—Ç—ã".
- –ï—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—à—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ ‚Äî –º—è–≥–∫–æ —É—Ç–æ—á–Ω–∏ –∏ –ø—Ä–æ–≤–µ—Ä—å.
- –°–º–µ—â–µ–Ω–∏—è: –µ—Å–ª–∏ –≤–∏–¥–∏—à—å "–Ω–∞–¥–æ/–¥–æ–ª–∂–µ–Ω/—Ä–∞–¥–∏ —Å–µ–º—å–∏", —Å–∏–ª—å–Ω—É—é —Ç—Ä–µ–≤–æ–≥—É, —Å–æ—Ü–∏–∞–ª—å–Ω–æ-–∏–¥–µ–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã ‚Äî –∑–∞–¥–∞–π 1‚Äì2 –≤–æ–ø—Ä–æ—Å–∞ –Ω–∞ —Å–º–µ—â–µ–Ω–∏–µ.

–¢–´ –û–ë–Ø–ó–ê–ù –≤–µ—Ä–Ω—É—Ç—å –°–¢–†–û–ì–û JSON –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞, –≤ —Ñ–æ—Ä–º–∞—Ç–µ:

{
  "finish": false,
  "stage": "stage0_intake|stage1_now|stage2_childhood|stage3_hypothesis|stage4_shifts|stage5_wrap",
  "intent": "ask_name|ask_request|current_state|energy_source|childhood|behavior|hypothesis_check|shift_probe|wrap",
  "question_id": "string",
  "question_text": "string",
  "answer_type": "text|single|multi",
  "options": ["..."],             // —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ single/multi
  "allow_comment": true,          // –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ
  "running_hypothesis": {
    "top_candidates": ["..."],    // –ª—é–±—ã–µ –∏–∑ 9 –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–≤
    "notes": ["..."]              // 1-3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–º–µ—Ç–∫–∏
  },
  "client_preview": {             // –∑–∞–ø–æ–ª–Ω—è–π —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ finish=true (–∏–Ω–∞—á–µ null)
    "name": "...",
    "request": "...",
    "top3_hypothesis": ["..."],
    "fills_energy": ["..."],
    "drains_energy": ["..."],
    "next_step": "..."
  }
}

–ü—Ä–∞–≤–∏–ª–∞:
- options –º–∞–∫—Å–∏–º—É–º 6, –¥–æ–±–∞–≤–ª—è–π –≤–∞—Ä–∏–∞–Ω—Ç "–î—Ä—É–≥–æ–µ (–Ω–∞–ø–∏—à—É)" –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ.
- question_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –ø–æ —Å–º—ã—Å–ª—É (–Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π).
- –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—ã –≤–Ω–µ —Å–ø–∏—Å–∫–∞ 9.
"""


def build_user_prompt(cfg: Dict[str, Any], state: Dict[str, Any], retrieved: List[Chunk]) -> str:
    name = state.get("name") or ""
    request = state.get("request") or ""
    turn = state.get("turn", 0)
    max_turns = cfg["diagnosis"]["max_turns"]

    # –ò—Å—Ç–æ—Ä–∏—è (–∫–æ—Ä–æ—Ç–∫–æ)
    history_lines = []
    for ev in state.get("log", []):
        q = ev.get("question_text", "")
        a = ev.get("answer", "")
        history_lines.append(f"- Q: {q}\n  A: {a}")
    history = "\n".join(history_lines[-12:])  # –Ω–µ —Ä–∞–∑–¥—É–≤–∞–µ–º

    # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –∫—É—Å–∫–∏
    ctx_blocks = []
    for ch in retrieved:
        ctx_blocks.append(f"[{ch.source}#{ch.idx}]\n{ch.text}")
    ctx = "\n\n".join(ctx_blocks)

    # –ö–æ—Ä–æ—Ç–∫–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã
    rh = state.get("running_hypothesis") or {}
    top_cand = rh.get("top_candidates", [])
    notes = rh.get("notes", [])

    return f"""
–ö–û–ù–§–ò–ì:
- max_turns: {max_turns}
- current_turn: {turn}

–ö–õ–ò–ï–ù–¢:
- name: {name}
- request: {request}

–ò–°–¢–û–†–ò–Ø (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —à–∞–≥–∏):
{history if history else "- (–ø–æ–∫–∞ –Ω–µ—Ç)"}

–¢–ï–ö–£–©–ê–Ø –ì–ò–ü–û–¢–ï–ó–ê (–µ—Å–ª–∏ –µ—Å—Ç—å):
- top_candidates: {top_cand}
- notes: {notes}

–î–û–ö–£–ú–ï–ù–¢–´ (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫—É—Å–∫–∏ –∏–∑ knowledge/):
{ctx if ctx else "(–Ω–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫—É—Å–∫–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É ‚Äî –∑–∞–¥–∞–π –æ–±—â–∏–π, –Ω–æ —É–º–Ω—ã–π —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å)"}

–ó–ê–î–ê–ß–ê:
–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –°–õ–ï–î–£–Æ–©–ò–ô –≤–æ–ø—Ä–æ—Å –∏ —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (single/multi/text).
–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π —É–∂–µ –∑–∞–¥–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –î–≤–∏–≥–∞–π—Å—è –ø–æ –ª–æ–≥–∏–∫–µ –∂–∏–≤–æ–≥–æ —Ä–∞–∑–±–æ—Ä–∞:
—Å–∏—Ç—É–∞—Ü–∏—è —Å–µ–π—á–∞—Å ‚Üí –¥–µ—Ç—Å—Ç–≤–æ/–∏—Å—Ç–æ—Ä–∏—è ‚Üí –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑ ‚Üí (–µ—Å–ª–∏ –Ω–∞–¥–æ) —Å–º–µ—â–µ–Ω–∏—è ‚Üí –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ.
–ï—Å–ª–∏ turn —É–∂–µ >= {cfg["diagnosis"]["stop_rules"]["soft_stop_if_confident_after_turn"]}, –º–æ–∂–µ—à—å –∑–∞–≤–µ—Ä—à–∞—Ç—å, –µ—Å–ª–∏ –≥–∏–ø–æ—Ç–µ–∑–∞ —É—Å—Ç–æ–π—á–∏–≤–∞.
"""


def call_llm(cfg: Dict[str, Any], user_prompt: str) -> Dict[str, Any]:
    client = get_openai_client()
    model = cfg["llm"]["model"]
    temperature = cfg["llm"].get("temperature", 0.5)
    max_output_tokens = cfg["llm"].get("max_output_tokens", 450)

    max_retries = cfg["llm"]["retry"]["max_retries"]
    base_sleep = cfg["llm"]["retry"]["base_sleep_seconds"]
    max_sleep = cfg["llm"]["retry"]["max_sleep_seconds"]

    last_err = None
    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model=model,
                temperature=temperature,
                max_tokens=max_output_tokens,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
            )
            text = resp.choices[0].message.content or ""
            data = safe_json_loads(text)
            if not data:
                raise ValueError("Model did not return valid JSON.")
            return data
        except RateLimitError as e:
            last_err = e
            sleep = min(max_sleep, base_sleep * (2 ** attempt))
            time.sleep(sleep)
        except (APITimeoutError, APIError, ValueError) as e:
            last_err = e
            sleep = min(max_sleep, base_sleep * (2 ** attempt))
            time.sleep(sleep)

    raise RuntimeError(f"LLM error after retries: {last_err}")


# ----------------------------
# Streamlit state
# ----------------------------
def init_state():
    st.session_state.setdefault("turn", 0)
    st.session_state.setdefault("log", [])  # list of events
    st.session_state.setdefault("current", None)  # current question dict from LLM
    st.session_state.setdefault("name", "")
    st.session_state.setdefault("request", "")
    st.session_state.setdefault("running_hypothesis", {"top_candidates": [], "notes": []})
    st.session_state.setdefault("finished", False)
    st.session_state.setdefault("client_preview", None)
    st.session_state.setdefault("debug_last_raw", None)
    st.session_state.setdefault("debug_retrieved_titles", [])
    st.session_state.setdefault("debug_scored_preview", [])


def reset_all():
    for k in list(st.session_state.keys()):
        if k.startswith("_"):
            continue
        del st.session_state[k]


# ----------------------------
# UI helpers
# ----------------------------
def render_question(q: Dict[str, Any]) -> Tuple[Optional[Any], bool]:
    st.caption(q.get("stage", ""))
    st.subheader(q.get("question_text", ""))
    a_type = q.get("answer_type", "text")
    opts = q.get("options") or []

    answer = None
    submitted = False

    if a_type == "single":
        answer = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ:", opts, index=0 if opts else None)
    elif a_type == "multi":
        answer = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ:", opts)
    else:
        answer = st.text_area("–û—Ç–≤–µ—Ç:", height=140, placeholder="–ú–æ–∂–Ω–æ –∫–æ—Ä–æ—Ç–∫–æ. –ï—Å–ª–∏ –µ—Å—Ç—å ‚Äî –æ–¥–∏–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä.")

    col1, col2 = st.columns([1, 1])
    with col1:
        submitted = st.button("–î–∞–ª–µ–µ ‚ûú", use_container_width=True)
    with col2:
        if st.button("–ü–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ –ò–ò", use_container_width=True):
            # –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏–º –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ç–æ–º –∂–µ turn
            st.session_state["current"] = None
            st.rerun()

    return answer, submitted


def build_retrieval_query(state: Dict[str, Any]) -> str:
    parts = []
    if state.get("request"):
        parts.append(state["request"])
    # –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç
    if state.get("log"):
        parts.append(str(state["log"][-1].get("answer", "")))
    # –≥–∏–ø–æ—Ç–µ–∑–∞
    rh = state.get("running_hypothesis") or {}
    tc = rh.get("top_candidates", [])
    if tc:
        parts.append(" ".join(tc))
    return " ".join(parts).strip()


# ----------------------------
# Main
# ----------------------------
st.set_page_config(page_title="NEO –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–≤ (MVP)", layout="centered")

cfg = load_json("configs/diagnosis_config.json")
init_state()

st.title("NEO –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–≤")
st.write("–§–æ—Ä–º–∞—Ç: –∂–∏–≤–æ–π —Ä–∞–∑–±–æ—Ä. –í–æ–ø—Ä–æ—Å—ã —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ò–ò –ø–æ –ª–æ–≥–∏–∫–µ —ç—Ç–∞–ø–æ–≤, –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤.")

topbar1, topbar2 = st.columns([1, 1])
with topbar1:
    st.write(f"–•–æ–¥: –≤–æ–ø—Ä–æ—Å {st.session_state['turn'] + 1} –∏–∑ {cfg['diagnosis']['max_turns']}")
with topbar2:
    if st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É"):
        reset_all()
        st.rerun()

# preload knowledge chunks
retr_cfg = cfg["retrieval"]
knowledge_chunks = load_knowledge_chunks(
    retr_cfg["knowledge_dir"],
    retr_cfg["files"],
    retr_cfg["chunking"]["max_chars_per_chunk"],
    retr_cfg["chunking"]["overlap_chars"],
)

# If finished, show result
if st.session_state.get("finished"):
    st.success("–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ‚úÖ")
    preview = st.session_state.get("client_preview") or {}
    st.markdown(f"**–ò–º—è:** {preview.get('name','')}")
    st.markdown(f"**–ó–∞–ø—Ä–æ—Å:** {preview.get('request','')}")
    top3 = preview.get("top3_hypothesis") or []
    st.markdown("**–ì–∏–ø–æ—Ç–µ–∑–∞ (—Ç–æ–ø-3 –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞):** " + (", ".join(top3) if top3 else "‚Äî"))

    fills = preview.get("fills_energy") or []
    drains = preview.get("drains_energy") or []
    if fills:
        st.markdown("**–ß—Ç–æ –≤–∞—Å –Ω–∞–ø–æ–ª–Ω—è–µ—Ç:**")
        for x in fills[:6]:
            st.write(f"‚Ä¢ {x}")
    if drains:
        st.markdown("**–ß—Ç–æ –∑–∞–±–∏—Ä–∞–µ—Ç —ç–Ω–µ—Ä–≥–∏—é:**")
        for x in drains[:6]:
            st.write(f"‚Ä¢ {x}")

    st.markdown("**–ß—Ç–æ –¥–∞–ª—å—à–µ:**")
    st.write(preview.get("next_step", "–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ ‚Äî –º–∞—Å—Ç–µ—Ä—Å–∫–∞—è –≤–µ—Ä—Å–∏—è –æ—Ç—á—ë—Ç–∞."))

    with st.expander("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–¥–ª—è –º–∞—Å—Ç–µ—Ä–∞ / –æ—Ç–ª–∞–¥–∫–∏)"):
        st.json(st.session_state.get("log", []))
        st.json({"running_hypothesis": st.session_state.get("running_hypothesis")})
        st.json({"retrieved": st.session_state.get("debug_retrieved_titles")})
        if st.session_state.get("debug_last_raw"):
            st.json(st.session_state["debug_last_raw"])
    st.stop()

# Generate next question if needed
if st.session_state.get("current") is None:
    # Stop guard by hard limit
    if st.session_state["turn"] >= cfg["diagnosis"]["max_turns"]:
        st.session_state["finished"] = True
        st.session_state["client_preview"] = {
            "name": st.session_state.get("name", ""),
            "request": st.session_state.get("request", ""),
            "top3_hypothesis": (st.session_state.get("running_hypothesis") or {}).get("top_candidates", [])[:3],
            "fills_energy": [],
            "drains_energy": [],
            "next_step": "–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ ‚Äî –º–∞—Å—Ç–µ—Ä—Å–∫–∞—è –≤–µ—Ä—Å–∏—è –æ—Ç—á—ë—Ç–∞ (–¥–µ—Ç–∞–ª–∏, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, –¥–µ–Ω—å–≥–∏, –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π)."
        }
        st.rerun()

    # Retrieval
    query = build_retrieval_query(st.session_state)
    retrieved, scored_preview = retrieve_chunks(
        knowledge_chunks,
        query=query,
        top_k=retr_cfg["top_k"],
        max_total_chars=retr_cfg["max_context_chars_total"],
    )
    st.session_state["debug_retrieved_titles"] = [f"{c.source}#{c.idx}" for c in retrieved]
    st.session_state["debug_scored_preview"] = [
        {"source": f"{c.source}#{c.idx}", "score": s} for c, s in scored_preview[:10]
    ]

    # Build prompt and call LLM
    user_prompt = build_user_prompt(cfg, st.session_state, retrieved)

    try:
        data = call_llm(cfg, user_prompt)
        st.session_state["debug_last_raw"] = data

        # basic validation and anti-repeat guard
        asked_texts = set(normalize_text(ev.get("question_text", "")) for ev in st.session_state.get("log", []))
        qtext = data.get("question_text", "")
        if normalize_text(qtext) in asked_texts and not data.get("finish", False):
            # If repeated, force regenerate once by clearing current and rerun
            st.warning("–ò–ò –ø–æ–ø—ã—Ç–∞–ª—Å—è –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –≤–æ–ø—Ä–æ—Å ‚Äî –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä—É—é‚Ä¶")
            st.session_state["current"] = None
            # tiny perturbation: append to logless state? simplest: add note to hypothesis
            rh = st.session_state.get("running_hypothesis") or {"top_candidates": [], "notes": []}
            rh["notes"] = (rh.get("notes") or []) + ["–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –∑–∞–¥–∞–π –Ω–æ–≤—ã–π —É—Ç–æ—á–Ω—è—é—â–∏–π."]
            st.session_state["running_hypothesis"] = rh
            st.rerun()

        st.session_state["current"] = data
    except Exception as e:
        msg = str(e)
        # If rate limit ‚Äì show friendly
        if "rate" in msg.lower() or "429" in msg:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–æ–ø—Ä–æ—Å –æ—Ç –ò–ò: –ª–∏–º–∏—Ç/429. –ù–∞–∂–º–∏ ¬´–ü–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ –ò–ò¬ª —á–µ—Ä–µ–∑ –ø–∞—Ä—É —Å–µ–∫—É–Ω–¥.")
        else:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–æ–ø—Ä–æ—Å –æ—Ç –ò–ò: {e}")
        with st.expander("Debug"):
            st.code(user_prompt[:4000] + ("\n...\n" if len(user_prompt) > 4000 else ""))
        st.stop()

# Render current question
q = st.session_state["current"]

# If model says finish now
if q.get("finish", False):
    st.session_state["finished"] = True
    st.session_state["client_preview"] = q.get("client_preview") or {}
    # ensure name/request
    st.session_state["client_preview"]["name"] = st.session_state.get("name", st.session_state["client_preview"].get("name", ""))
    st.session_state["client_preview"]["request"] = st.session_state.get("request", st.session_state["client_preview"].get("request", ""))
    st.rerun()

answer, submitted = render_question(q)

if submitted:
    # Validate empty answers a bit
    if q.get("answer_type") in ("single", "text") and (answer is None or str(answer).strip() == ""):
        st.warning("–û—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π. –í—ã–±–µ—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç –∏–ª–∏ –Ω–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ.")
        st.stop()

    # persist intake fields if applicable
    intent = q.get("intent", "")
    if intent == "ask_name":
        st.session_state["name"] = str(answer).strip()
    if intent == "ask_request":
        st.session_state["request"] = str(answer).strip()

    # update running hypothesis from LLM output
    rh = q.get("running_hypothesis")
    if isinstance(rh, dict):
        st.session_state["running_hypothesis"] = {
            "top_candidates": rh.get("top_candidates", [])[:9],
            "notes": rh.get("notes", [])[:6],
        }

    # log event
    st.session_state["log"].append(
        {
            "turn": st.session_state["turn"],
            "question_id": q.get("question_id", f"turn_{st.session_state['turn']}"),
            "intent": intent,
            "stage": q.get("stage", ""),
            "question_text": q.get("question_text", ""),
            "answer": answer,
        }
    )

    st.session_state["turn"] += 1
    st.session_state["current"] = None
    st.rerun()

# Debug expander
if cfg["output"]["debug_panel"]["enabled"]:
    with st.expander("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–¥–ª—è –º–∞—Å—Ç–µ—Ä–∞ / –æ—Ç–ª–∞–¥–∫–∏)"):
        st.write("retrieved:", st.session_state.get("debug_retrieved_titles"))
        st.write("scored preview:", st.session_state.get("debug_scored_preview"))
        st.json({"running_hypothesis": st.session_state.get("running_hypothesis")})
        st.json({"turn_log_tail": st.session_state.get("log", [])[-5:]})